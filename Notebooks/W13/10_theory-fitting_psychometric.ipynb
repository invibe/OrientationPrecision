{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to compare methods which fit data with psychometric curves using logistic regression. Indeed, after (long) experiments where for instance you collected sequences of keypresses, it is important to infer at best the parameters of the underlying processes: was the observer biased, was she more precise? While I was *forevever* using [sklearn](https://scikit-learn.org/stable/index.html) and praised it's beautifully crafted methods, I lacked some flexibility in the definition of the model. This notebook was done in collaboration with [Jenna Fradin](https://github.com/jennafradin), master student in the lab.\n",
    "\n",
    "Here, I define a similar fitting method using [pytorch](https://pytorch.org/) which fits in a few lines of code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "criterion = torch.nn.BCELoss(reduction='sum')\n",
    "\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, bias=True, logit0=-1): \n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1, bias=bias)    \n",
    "        self.logit0 = torch.nn.Parameter(logit0*torch.ones(1))\n",
    "        \n",
    "    def forward(self, theta):\n",
    "        p0 = torch.sigmoid(self.logit0)\n",
    "        out = p0/2 + (1-p0)*torch.sigmoid(self.linear(theta))\n",
    "        return out\n",
    "\n",
    "learning_rate = 0.05\n",
    "beta1, beta2 = 0.99, 0.99\n",
    "betas = (beta1, beta2)\n",
    "num_epochs = 2**11 + 1\n",
    "batch_size = 128\n",
    "#gamma = .05 ** (1/num_epochs)\n",
    "\n",
    "def fit_data(theta, y,\n",
    "                learning_rate=learning_rate, batch_size=batch_size, #gamma=gamma,\n",
    "                num_epochs=num_epochs, betas=betas,\n",
    "                verbose=False):\n",
    "\n",
    "    Theta, labels = torch.Tensor(theta[:, None]), torch.Tensor(y[:, None])\n",
    "    loader = DataLoader(TensorDataset(Theta, labels), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    logistic_model = LogisticRegressionModel()\n",
    "    logistic_model = logistic_model.to(device)\n",
    "    logistic_model.train()\n",
    "    optimizer = torch.optim.Adam(logistic_model.parameters(), lr=learning_rate, betas=betas, amsgrad=False)\n",
    "    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "    \n",
    "    for epoch in range(int(num_epochs)):\n",
    "        logistic_model.train()\n",
    "        losses = []\n",
    "        for Theta_, labels_ in loader:\n",
    "            Theta_, labels_ = Theta_.to(device), labels_.to(device)\n",
    "            outputs = logistic_model(Theta_)\n",
    "            loss = criterion(outputs, labels_)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        if verbose and (epoch % (num_epochs//32) == 0) : \n",
    "            print(f\"Iteration: {epoch} - Loss: {np.sum(losses)/len(theta):.5f}\")\n",
    "        #scheduler.step()\n",
    "        \n",
    "    logistic_model.eval()\n",
    "    Theta, labels = torch.Tensor(theta[:, None]), torch.Tensor(y[:, None])\n",
    "    outputs = logistic_model(Theta)\n",
    "    loss = criterion(outputs, labels).item()/len(theta)\n",
    "    return logistic_model, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and run a series of tests to compare both methods.\n",
    "\n",
    "<!-- TEASER_END -->\n",
    "\n",
    "Let's first initialize the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/usr/local/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "from pylab import rcParams\n",
    "#print(rcParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 20\n",
    "rcParams['font.size'] = fontsize\n",
    "rcParams['legend.fontsize'] = fontsize\n",
    "rcParams['axes.labelsize'] = fontsize\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hyper parameters which we will tune later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256\n",
    "#batch_size = N//4\n",
    "#batch_size = N//2\n",
    "\n",
    "N_cv = 10\n",
    "# N_cv = 2\n",
    "\n",
    "seed = 42\n",
    "N_scan = 9\n",
    "N_test = 2000 # number of points for validation\n",
    "\n",
    "bias = True\n",
    "\n",
    "p0 = 0.1       \n",
    "theta0 = 0.\n",
    "wt = np.pi/32\n",
    "theta_std = np.pi/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## problem statement: a 2aFC task on synthetic data\n",
    "\n",
    "We will generate a typical setup where we have to guess for the otientation of a visual display compared to the vertical and ask observer to either press on the `left` or `right` arrows. The visual display will be controlled by a $theta$ parameter which we draw randomly according to a Gaussian probability density function. This may be synthesized in the following *psychometric* function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psychometric_function(theta, p0=p0, theta0=theta0, wt=wt):\n",
    "    return p0/2 + (1-p0) / (1+np.exp(-(theta-theta0)/wt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "such that we can draw the data according to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(\n",
    "            N = N,\n",
    "            p0 = p0,\n",
    "            theta0 = theta0,\n",
    "            wt = wt,\n",
    "            theta_std = theta_std,\n",
    "            seed=seed):    \n",
    "    np.random.seed(seed)\n",
    "    # theta = np.random.randn(N)*theta_std\n",
    "    theta = (2*np.random.rand(N)-1)*theta_std\n",
    "    \n",
    "    p = psychometric_function(theta, p0, theta0, wt)\n",
    "\n",
    "    y = np.random.rand(N) < p #generate data\n",
    "    return theta, p, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 µs ± 18.4 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "theta, p, y = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 - Loss: 0.66975\n",
      "Iteration: 64 - Loss: 0.30060\n",
      "Iteration: 128 - Loss: 0.26881\n",
      "Iteration: 192 - Loss: 0.25819\n",
      "Iteration: 256 - Loss: 0.25424\n",
      "Iteration: 320 - Loss: 0.25338\n",
      "Iteration: 384 - Loss: 0.25247\n",
      "Iteration: 448 - Loss: 0.25221\n",
      "Iteration: 512 - Loss: 0.25218\n",
      "Iteration: 576 - Loss: 0.25216\n",
      "Iteration: 640 - Loss: 0.25215\n",
      "Iteration: 704 - Loss: 0.25215\n",
      "Iteration: 768 - Loss: 0.25215\n",
      "Iteration: 832 - Loss: 0.25216\n",
      "Iteration: 896 - Loss: 0.25216\n",
      "Iteration: 960 - Loss: 0.25215\n",
      "Iteration: 1024 - Loss: 0.25215\n",
      "Iteration: 1088 - Loss: 0.25215\n",
      "Iteration: 1152 - Loss: 0.25216\n",
      "Iteration: 1216 - Loss: 0.25215\n",
      "Iteration: 1280 - Loss: 0.25215\n",
      "Iteration: 1344 - Loss: 0.25215\n",
      "Iteration: 1408 - Loss: 0.25216\n",
      "Iteration: 1472 - Loss: 0.25217\n",
      "Iteration: 1536 - Loss: 0.25215\n",
      "Iteration: 1600 - Loss: 0.25215\n",
      "Iteration: 1664 - Loss: 0.25216\n",
      "Iteration: 1728 - Loss: 0.25215\n",
      "Iteration: 1792 - Loss: 0.25215\n",
      "Iteration: 1856 - Loss: 0.25215\n",
      "Iteration: 1920 - Loss: 0.25216\n",
      "Iteration: 1984 - Loss: 0.25215\n",
      "Iteration: 2048 - Loss: 0.25215\n",
      "Final loss = 0.2521462984510656\n"
     ]
    }
   ],
   "source": [
    "theta, p, y = get_data()\n",
    "logistic_model, loss = fit_data(theta, y, verbose=True)\n",
    "print(\"Final loss =\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0 = 0.100, theta0 = 0.000, wt = 0.098, theta_std = 1.571\n"
     ]
    }
   ],
   "source": [
    "print(f'p0 = {p0:.3f}, theta0 = {theta0:.3f}, wt = {wt:.3f}, theta_std = {theta_std:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta0 = 0.047\n",
      "slope = 0.064\n",
      "p0 = 0.108\n"
     ]
    }
   ],
   "source": [
    "def get_params(logistic_model, verbose=False):\n",
    "    theta0_ = -logistic_model.linear.bias.item()/logistic_model.linear.weight.item()\n",
    "    wt_ = 1/logistic_model.linear.weight.item()\n",
    "    p0_ = torch.sigmoid(logistic_model.logit0).item()\n",
    "    \n",
    "    if verbose:\n",
    "        if bias: print(f'theta0 = {theta0_:.3f}' )\n",
    "        print(f'slope = {wt_:.3f}')    \n",
    "        print(f'p0 = {p0_:.3f}')        \n",
    "    return theta0_, wt_, p0_\n",
    "\n",
    "theta0_, wt_, p0_ = get_params(logistic_model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That method is fairly quick, in under 2 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "logistic_model, loss = fit_data(theta, y, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's do the same thing with `sklearn`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "tol = 1.e-4\n",
    "C = 3.0\n",
    "def fit_data_sklearn(theta, y, \n",
    "                num_epochs=num_epochs,\n",
    "                tol=tol, C=C, \n",
    "                verbose=False):\n",
    "    logistic_model = LogisticRegression(solver='liblinear', max_iter=num_epochs, C=C, tol=tol, fit_intercept=True)\n",
    "    logistic_model.fit(theta[:, None], y)\n",
    "    \n",
    "    outputs = logistic_model.predict_proba(theta[:, None])[:, 1]\n",
    "    outputs_, labels = torch.Tensor(outputs[:, None]), torch.Tensor(y[:, None])\n",
    "    loss = criterion(outputs_, labels).item()/len(theta)\n",
    "    if verbose: print(\"Loss =\", loss)\n",
    "    return logistic_model, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model_sk, loss = fit_data_sklearn(theta, y, verbose=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_sk(logistic_model, verbose=False):\n",
    "    \n",
    "    theta0_ = -logistic_model.intercept_[0]/logistic_model.coef_[0][0]\n",
    "    wt_ = 1/logistic_model.coef_[0][0]\n",
    "\n",
    "    if verbose:\n",
    "        if bias: print(f'theta0 = {theta0_:.3f}' )\n",
    "        print(f'slope = {wt_:.3f}')    \n",
    "    return theta0_, wt_\n",
    "\n",
    "theta0_, wt_ = get_params_sk(logistic_model_sk, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That method is *much* quicker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "logistic_model_sk, loss = fit_data_sklearn(theta, y, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... but what is the value of few seconds after hours of having observers sitting in front of a screen looking at (often boring) visual displays? More seriously, most important is the reliability of the values which are inferred by each respective method, such that they are correctly reflecting the information contained in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qualitative comparison of methods\n",
    "\n",
    "We can synthesize this comparison by drawing a new dataset and plotting the psychometric curves which are obtained by each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, p, y = get_data() # nouvelles données \n",
    "logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "print(f'Training loss = {loss:.3f}')\n",
    "logistic_model_sk, loss_sk = fit_data_sklearn(theta, y, verbose=False)\n",
    "print(f'Training sklearn loss = {loss_sk:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.scatter(theta, y, s=6, alpha=.4, color = 'k', label='data points')\n",
    "# ax.scatter(theta, p, s=6, alpha=.4, color = 'b', label='hidden proba')\n",
    "x_values = np.linspace(-1.5, 1.50, 100)[:, None]\n",
    "y_values_p = psychometric_function(x_values, p0, theta0, wt)\n",
    "ax.plot(x_values, y_values_p, alpha=.4, color = 'b', label='hidden proba')\n",
    "y_values = logistic_model(torch.Tensor(x_values)).detach().numpy()\n",
    "ax.plot(x_values, y_values, 'g', alpha=.7, lw=3, label='torch')\n",
    "y_values_sk = logistic_model_sk.predict_proba(x_values)[:, 1]\n",
    "ax.plot(x_values, y_values_sk, 'r', alpha=.7, lw=3, label='sklearn')\n",
    "ax.set_xlabel(r'orientation $\\theta$', fontsize=20)\n",
    "ax.set_yticks([0.,1.])\n",
    "ax.set_yticklabels(['Left', 'Right'], fontsize=20)\n",
    "plt.legend(fontsize=20, frameon=False, scatterpoints=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The losses which were computed above are those obtained during training. Relying on this value may be a dangerous strategy as the model may be overfitting the data. We should therefore measure how the model would generalize with novel data.\n",
    "\n",
    "While it hard to do with real (experimental) data which are often scarse, here we synthesized the data and we can thus compute a testing loss by drawing again a set of new data and computing the loss on that data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, p, y = get_data(N=N_test, seed=seed+N_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_true(theta, p, y):\n",
    "    labels = torch.Tensor(y[:, None])        \n",
    "    P = torch.Tensor(p[:, None])\n",
    "    return criterion(P, labels).item()/len(theta)\n",
    "print(f'Testing true loss = {loss_true(theta, p, y):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "print(f'Training loss = {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_torch(logistic_model, theta, p, y):\n",
    "    labels = torch.Tensor(y[:, None])\n",
    "    Theta = torch.Tensor(theta[:, None])\n",
    "    P = torch.Tensor(p[:, None])\n",
    "\n",
    "    outputs = logistic_model(Theta)\n",
    "    return criterion(outputs, labels).item()/len(theta)\n",
    "print(f'Testing loss = {loss_torch(logistic_model, theta, p, y):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model_sk, loss_sk = fit_data_sklearn(theta, y, verbose=False)\n",
    "print(f'Training sklearn loss = {loss_sk:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_sklearn(logistic_model, theta, p, y):\n",
    "    outputs = logistic_model.predict_proba(theta[:, None])[:, 1]\n",
    "    outputs_, labels = torch.Tensor(outputs[:, None]), torch.Tensor(y[:, None])\n",
    "    return criterion(outputs_, labels).item()/len(theta)\n",
    "\n",
    "print(f'Testing sklearn loss = {loss_sklearn(logistic_model_sk, theta, p, y):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## quantitative comparison of methods : varrying methods' parameters\n",
    "\n",
    "Let's study the influence of each method's meta-parameter, such as the number of iterations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = learning_rate * np.logspace(-1, 1, N_scan, base=10)\n",
    "learning_rates_, losses, loss_Ps = [], [], []\n",
    "for learning_rate_ in learning_rates:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(seed=seed+i_CV)\n",
    "\n",
    "        logistic_model, loss = fit_data(theta, y, learning_rate=learning_rate_, verbose=False)        \n",
    "\n",
    "        if N_test>0:\n",
    "            theta, p, y = get_data(N=N_test, seed=seed+i_CV+N_test)\n",
    "            loss = loss_torch(logistic_model, theta, p, y)            \n",
    "        loss_P = loss_true(theta, p, y)\n",
    "        \n",
    "        if i_CV==0: \n",
    "            print(f\"learning_rate: {learning_rate_:.5f}, Loss: {loss:.5f}, loss_P: {loss_P:.5f}\")\n",
    "        learning_rates_.append(learning_rate_)\n",
    "        loss_Ps.append(loss_P/loss_P)\n",
    "        losses.append(loss/loss_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = dict(marker='.', lw=0, alpha=3/N_cv, ms=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#influence du learning rate sur loss\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15, 8))\n",
    "ax.plot(learning_rates_, losses, **opts, color='green', label='loss')\n",
    "ax.plot(learning_rates_, loss_Ps, **opts, color='blue', label='true')\n",
    "\n",
    "#ax.set_xlim(np.min(learning_rates_), np.max(learning_rates_))\n",
    "\n",
    "ax.set_xlabel('learning_rate')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence du nombre d'epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochss = num_epochs * np.logspace(-2, 0, N_scan, base=10)\n",
    "num_epochss_, losses, loss_Ps, loss_SKLs = [], [], [], []\n",
    "for num_epochs_ in num_epochss:\n",
    "    for i_CV in range(N_cv):\n",
    "        \n",
    "        theta, p, y = get_data(seed=seed+i_CV)\n",
    "        logistic_model, loss = fit_data(theta, y, num_epochs=int(num_epochs_), verbose=False)\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, num_epochs=int(num_epochs_), verbose=False)\n",
    "        \n",
    "        if N_test>0:\n",
    "            theta, p, y = get_data(N=N_test, seed=seed+i_CV+N_test)\n",
    "            loss = loss_torch(logistic_model, theta, p, y)            \n",
    "            loss_SKL = loss_sklearn(logistic_model_sk, theta, p, y)\n",
    "        loss_P = loss_true(theta, p, y)\n",
    "                \n",
    "        if i_CV==0: \n",
    "            print(f\"num_epochs: {int(num_epochs_)}, Loss: {loss:.5f}, loss_P: {loss_P:.5f}, loss_SKL: {loss_SKL:.5f}\")\n",
    "        num_epochss_.append(num_epochs_)\n",
    "        loss_SKLs.append(loss_SKL/loss_P)\n",
    "        loss_Ps.append(loss_P/loss_P)\n",
    "        losses.append(loss/loss_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# influence du nbr d'epochs sur loss \n",
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.plot(num_epochss_, losses, **opts, color='green', label='loss')\n",
    "ax.plot(num_epochss_, loss_Ps, **opts, color='blue', label='loss_P')\n",
    "ax.plot(num_epochss_, loss_SKLs, **opts, color='red', label='loss_SKL')\n",
    "\n",
    "ax.set_xlabel('# epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of minibatch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = N * np.logspace(-3, 0, N_scan, base=2)\n",
    "batch_sizes_, losses, loss_Ps = [], [], []\n",
    "timings = []\n",
    "for batch_size_ in batch_sizes:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(seed=seed+i_CV)\n",
    "        tic = time.time()\n",
    "        logistic_model, loss = fit_data(theta, y, batch_size=int(batch_size_), verbose=False)\n",
    "        toc = time.time()\n",
    "\n",
    "        if N_test>0:\n",
    "            theta, p, y = get_data(N=N_test, seed=seed+i_CV+N_test)\n",
    "            loss = loss_torch(logistic_model, theta, p, y)            \n",
    "        loss_P = loss_true(theta, p, y)\n",
    "        \n",
    "        if i_CV==0: \n",
    "            print(f\"batch_size: {int(batch_size_)}, Loss: {loss:.5f}, loss_P: {loss_P:.5f}, loss_SKL: {loss_SKL:.5f}, CPU time: {toc-tic:.1f}\")\n",
    "        batch_sizes_.append(batch_size_)\n",
    "        loss_Ps.append(loss_P/loss_P)\n",
    "        losses.append(loss/loss_P)\n",
    "        timings.append(toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.plot(batch_sizes_, losses, **opts, color='green', label='loss')\n",
    "ax.plot(batch_sizes_, loss_Ps, **opts, color='blue', label='true')\n",
    "\n",
    "ax.set_xlabel('batch_size')\n",
    "ax.set_ylabel('Loss ')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.plot(batch_sizes_, timings, **opts)\n",
    "ax.set_xlabel('batch_size')\n",
    "ax.set_ylabel('CPU time')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of `beta1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1s = 1. - np.logspace(-4, -1, N_scan, base=10, endpoint=True)\n",
    "beta1s_, losses, loss_Ps = [], [], []\n",
    "for beta1_ in beta1s:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(seed=seed+i_CV)\n",
    "        \n",
    "        logistic_model, loss = fit_data(theta, y, betas=(beta1_, beta2), verbose=False)\n",
    "\n",
    "        if N_test>0:\n",
    "            theta, p, y = get_data(N=N_test, seed=seed+i_CV+N_test)\n",
    "            loss = loss_torch(logistic_model, theta, p, y)            \n",
    "        loss_P = loss_true(theta, p, y)\n",
    "\n",
    "        if i_CV==0: \n",
    "            print(f\"beta1: {beta1_:.5f}, Loss: {loss:.5f}, loss_P: {loss_P:.5f}\")\n",
    "        beta1s_.append(beta1_)\n",
    "        loss_Ps.append(loss_P/loss_P)\n",
    "        losses.append(loss/loss_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.plot(beta1s_, losses, **opts, color='green', label='loss')\n",
    "ax.plot(beta1s_, loss_Ps, **opts, color='blue', label='true')\n",
    "\n",
    "ax.set_xlabel('beta1')\n",
    "ax.set_ylabel('Loss ')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of `beta2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta2s = 1. - np.logspace(-7, -1, N_scan, base=10, endpoint=True)\n",
    "beta2s_, losses, loss_Ps = [], [], []\n",
    "for beta2_ in beta2s:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(seed=seed+i_CV)\n",
    "\n",
    "        logistic_model, loss = fit_data(theta, y, betas=(beta1, beta2_), verbose=False)\n",
    "\n",
    "        if N_test>0:\n",
    "            theta, p, y = get_data(N=N_test, seed=seed+i_CV+N_test)\n",
    "            loss = loss_torch(logistic_model, theta, p, y)            \n",
    "        loss_P = loss_true(theta, p, y)\n",
    "        \n",
    "        if i_CV==0: \n",
    "            print(f\"beta2: {beta2_:.6f}, Loss: {loss:.5f}, loss_P: {loss_P:.5f}\")\n",
    "        beta2s_.append(beta2_)\n",
    "        loss_Ps.append(loss_P/loss_P)\n",
    "        losses.append(loss/loss_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.plot(beta2s_, losses, **opts, color='green', label='loss')\n",
    "ax.plot(beta2s_, loss_Ps, **opts, color='blue', label='true')\n",
    "\n",
    "ax.set_xlabel('beta2')\n",
    "ax.set_ylabel('Loss ')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of `C`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = C * np.logspace(-2, 2, N_scan, base=4)\n",
    "Cs_, loss_Ps, loss_SKLs = [], [], []\n",
    "for C_ in Cs:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(seed=seed+i_CV)\n",
    "\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, C=C_, verbose=False)\n",
    "\n",
    "        if N_test>0:\n",
    "            theta, p, y = get_data(N=N_test, seed=seed+i_CV+N_test)\n",
    "            loss_SKL = loss_sklearn(logistic_model_sk, theta, p, y)\n",
    "        loss_P = loss_true(theta, p, y)\n",
    "\n",
    "        if i_CV==0: \n",
    "            print(f\"C: {C_:.5f}, loss_P: {loss_P:.5f}, loss_SKL: {loss_SKL:.5f}\")\n",
    "            \n",
    "        Cs_.append(C_)\n",
    "        loss_SKLs.append(loss_SKL/loss_P)\n",
    "        loss_Ps.append(loss_P/loss_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.plot(Cs_, loss_Ps, **opts, color='blue', label='true')\n",
    "ax.plot(Cs_, loss_SKLs, **opts, color='red', label='loss_SKL')\n",
    "\n",
    "ax.set_xlabel('C')\n",
    "ax.set_ylabel('Loss ')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of `tol`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tols = tol * np.logspace(-2, 2, N_scan, base=10)\n",
    "tols_, loss_Ps, loss_SKLs = [], [], []\n",
    "for tol_ in tols:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(seed=seed+i_CV)\n",
    "\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, tol=tol_, verbose=False)\n",
    "        \n",
    "        if N_test>0:\n",
    "            theta, p, y = get_data(N=N_test, seed=seed+i_CV+N_test)\n",
    "            loss_SKL = loss_sklearn(logistic_model_sk, theta, p, y)\n",
    "        loss_P = loss_true(theta, p, y)\n",
    "\n",
    "        if i_CV==0: \n",
    "            print(f\"tol: {tol_:.5f}, loss_P: {loss_P:.5f}, loss_SKL: {loss_SKL:.5f}\")\n",
    "        tols_.append(tol_)\n",
    "        loss_SKLs.append(loss_SKL/loss_P)\n",
    "        loss_Ps.append(loss_P/loss_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.plot(tols_, loss_Ps, **opts, color='blue', label='true')\n",
    "ax.plot(tols_, loss_SKLs, **opts, color='red', label='loss_SKL')\n",
    "\n",
    "ax.set_xlabel('tol')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xscale('log')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## quantitative comparison of methods : varrying experimental parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of number of trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = np.logspace(1.5, 3, N_scan, base=10, endpoint=True)\n",
    "\n",
    "Ns_, losses, loss_Ps, loss_SKLs = [], [], [], []\n",
    "\n",
    "for N_ in Ns:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(N=int(N_), seed=seed+i_CV)\n",
    "        \n",
    "        logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, verbose=False)\n",
    "\n",
    "        if N_test>0:\n",
    "            theta, p, y = get_data(N=N_test, seed=seed+i_CV+N_test)\n",
    "            loss = loss_torch(logistic_model, theta, p, y)            \n",
    "            loss_SKL = loss_sklearn(logistic_model_sk, theta, p, y)\n",
    "        loss_P = loss_true(theta, p, y)\n",
    "\n",
    "        if i_CV==0: print(f\"N: {int(N_)}, Loss: {loss:.5f}, loss_P: {loss_P:.5f}, loss_SKL: {loss_SKL:.5f}\")\n",
    "        loss_SKLs.append(loss_SKL/loss_P)\n",
    "        Ns_.append(N_)\n",
    "        loss_Ps.append(loss_P/loss_P)\n",
    "        losses.append(loss/loss_P)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.plot(Ns_, losses, **opts, color='green', label='loss')\n",
    "ax.plot(Ns_, loss_Ps, **opts, color='blue', label='loss_P')\n",
    "ax.plot(Ns_, loss_SKLs, **opts, color='red', label='loss_SKL')\n",
    "\n",
    "ax.set_xlabel(' # trials')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xscale('log')\n",
    "plt.legend(fontsize=20, frameon=False, scatterpoints=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of theta_std\n",
    "\n",
    "The convergence of the fitting procedure may also depend on the parametrers of the data which were set to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'p0 = {p0:.3f}, theta0 = {theta0:.3f}, wt = {wt:.3f}, theta_std = {theta_std:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_stds = theta_std * np.logspace(-1, 1, N_scan, base=2, endpoint=True)\n",
    "\n",
    "theta_stds_, losses, loss_Ps, loss_SKLs = [], [], [], []\n",
    "\n",
    "for theta_std_ in theta_stds:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(theta_std=theta_std_, seed=seed+i_CV)\n",
    "        \n",
    "        logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, verbose=False)\n",
    "\n",
    "        if N_test>0:\n",
    "            theta, p, y = get_data(theta_std=theta_std_, N=N_test, seed=seed+i_CV+N_test)\n",
    "            loss = loss_torch(logistic_model, theta, p, y)            \n",
    "            loss_SKL = loss_sklearn(logistic_model_sk, theta, p, y)\n",
    "        loss_P = loss_true(theta, p, y)\n",
    "\n",
    "        if i_CV==0: print(f\"theta_std: {theta_std_:.3f}, Loss: {loss:.5f}, loss_P: {loss_P:.5f}, loss_SKL: {loss_SKL:.5f}\")\n",
    "        loss_SKLs.append(loss_SKL/loss_P)\n",
    "        theta_stds_.append(theta_std_)\n",
    "        loss_Ps.append(loss_P/loss_P)\n",
    "        losses.append(loss/loss_P)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.plot(theta_stds_, losses, **opts, color='green', label='loss')\n",
    "ax.plot(theta_stds_, loss_Ps, **opts, color='blue', label='loss_P')\n",
    "ax.plot(theta_stds_, loss_SKLs, **opts, color='red', label='loss_SKL')\n",
    "\n",
    "ax.set_xlabel('theta_std')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xscale('log')\n",
    "plt.legend(fontsize=20, frameon=False, scatterpoints=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of `p0`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0s = np.logspace(-3, -.7, N_scan, base=10, endpoint=True)\n",
    "\n",
    "p0s_, losses, loss_Ps, loss_SKLs = [], [], [], []\n",
    "\n",
    "for p0_ in p0s:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(p0=p0_, seed=seed+i_CV)\n",
    "        \n",
    "        logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, verbose=False)\n",
    "        \n",
    "        if N_test>0:\n",
    "            theta, p, y = get_data(p0=p0_, N=N_test, seed=seed+i_CV)\n",
    "            loss = loss_torch(logistic_model, theta, p, y)\n",
    "            loss_SKL = loss_sklearn(logistic_model_sk, theta, p, y)\n",
    "        loss_P = loss_true(theta, p, y)\n",
    "        \n",
    "        if i_CV==0: print(f\"p0: {p0_:.3f}, Loss: {loss:.5f}, loss_P: {loss_P:.5f}, loss_SKL: {loss_SKL:.5f}\")\n",
    "        loss_SKLs.append(loss_SKL/loss_P)\n",
    "        p0s_.append(p0_)\n",
    "        loss_Ps.append(loss_P/loss_P)\n",
    "        losses.append(loss/loss_P)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15, 8)) \n",
    "ax.plot(p0s_, losses, **opts, color='green', label='loss')\n",
    "ax.plot(p0s_, loss_Ps, **opts, color='blue', label='loss_P')\n",
    "ax.plot(p0s_, loss_SKLs, **opts, color='red', label='loss_SKL')\n",
    "\n",
    "ax.set_xlabel('p0')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xscale('log')\n",
    "plt.legend(fontsize=20, frameon=False, scatterpoints=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## comparing the predicted values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'p0 = {p0:.3f}, theta0 = {theta0:.3f}, wt = {wt:.3f}, theta_std = {theta_std:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### changing `p0`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_scan = 20\n",
    "p0s = np.logspace(-3, -.7, N_scan, base=10, endpoint=True)\n",
    "\n",
    "p0s_, wts_, theta0s_, p0_tos, theta0_tos, theta0_sks, wt_tos, wt_sks = [], [], [], [], [], [], [], []\n",
    "\n",
    "for p0_ in p0s:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(p0=p0_, seed=seed+i_CV)\n",
    "        \n",
    "        logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, verbose=False)\n",
    "        \n",
    "        theta0_to, wt_to, p0_to = get_params(logistic_model, verbose=False)\n",
    "        theta0_sk, wt_sk = get_params_sk(logistic_model_sk, verbose=False)\n",
    "        \n",
    "        p0s_.append(p0_)\n",
    "        theta0s_.append(theta0)\n",
    "        wts_.append(wt)\n",
    "        p0_tos.append(p0_to)\n",
    "        theta0_tos.append(theta0_to)\n",
    "        theta0_sks.append(theta0_sk)\n",
    "        wt_tos.append(wt_to)\n",
    "        wt_sks.append(wt_sk)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 8))\n",
    "\n",
    "axs[0].scatter(p0s_, p0_tos, label='torch')\n",
    "axs[0].plot([min(p0s_), max(p0s_)], [min(p0_tos), max(p0_tos)], '--')\n",
    "axs[0].set(xlabel='p0 (true)', ylabel='p0 (predicted)')\n",
    "axs[0].legend(loc=\"upper left\")\n",
    "\n",
    "axs[1].scatter(p0s_, theta0_tos, label='torch')\n",
    "axs[1].scatter(p0s_, theta0_sks, label='sklearn')\n",
    "axs[1].plot([min(p0s_), max(p0s_)], [theta0, theta0], '--')\n",
    "axs[1].set(xlabel='p0', ylabel='theta0 (predicted)')\n",
    "axs[1].legend(loc=\"upper left\")\n",
    "\n",
    "axs[2].scatter(p0s_, wt_tos, label='torch')\n",
    "axs[2].scatter(p0s_, wt_sks, label='sklearn')\n",
    "axs[2].plot([min(p0s_), max(p0s_)], [wt, wt], '--')\n",
    "axs[2].set(xlabel='p0', ylabel='slope (predicted)')\n",
    "axs[2].legend(loc=\"upper left\")\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### changing `theta0`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_scan = 20\n",
    "theta0s = theta_std * np.linspace(-1, 1, N_scan, endpoint=True)\n",
    "\n",
    "p0s_, wts_, theta0s_, p0_tos, theta0_tos, theta0_sks, wt_tos, wt_sks = [], [], [], [], [], [], [], []\n",
    "\n",
    "for theta0_ in theta0s:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(theta0=theta0_, seed=seed+i_CV)\n",
    "        \n",
    "        logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, verbose=False)\n",
    "        \n",
    "        theta0_to, wt_to, p0_to = get_params(logistic_model, verbose=False)\n",
    "        theta0_sk, wt_sk = get_params_sk(logistic_model_sk, verbose=False)\n",
    "        \n",
    "        p0s_.append(p0)\n",
    "        theta0s_.append(theta0_)\n",
    "        wts_.append(wt)\n",
    "        p0_tos.append(p0_to)\n",
    "        theta0_tos.append(theta0_to)\n",
    "        theta0_sks.append(theta0_sk)\n",
    "        wt_tos.append(wt_to)\n",
    "        wt_sks.append(wt_sk)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 8))\n",
    "\n",
    "axs[0].scatter(theta0s_, p0_tos, label='torch')\n",
    "axs[0].plot([min(theta0s_), max(theta0s_)], [p0, p0], '--')\n",
    "axs[0].set(xlabel='theta0 (true)', ylabel='p0 (predicted)')\n",
    "axs[0].legend(loc=\"upper left\")\n",
    "\n",
    "axs[1].scatter(theta0s_, theta0_tos, label='torch')\n",
    "axs[1].scatter(theta0s_, theta0_sks, label='sklearn')\n",
    "axs[1].plot([min(theta0s_), max(theta0s_)], [min(theta0_sks), max(theta0_sks)], '--')\n",
    "axs[1].set(xlabel='theta0 (true)', ylabel='theta0 (predicted)')\n",
    "axs[1].legend(loc=\"upper left\")\n",
    "\n",
    "axs[2].scatter(theta0s_, wt_tos, label='torch')\n",
    "axs[2].scatter(theta0s_, wt_sks, label='sklearn')\n",
    "axs[2].plot([min(p0s_), max(p0s_)], [wt, wt], '--')\n",
    "axs[2].set(xlabel='theta0 (true)', ylabel='slope (predicted)')\n",
    "axs[2].legend(loc=\"upper left\")\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### changing `wt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_scan = 20\n",
    "wts = wt * np.logspace(-1, 1, N_scan, base=10, endpoint=True)\n",
    "\n",
    "p0s_, wts_, theta0s_, p0_tos, theta0_tos, theta0_sks, wt_tos, wt_sks = [], [], [], [], [], [], [], []\n",
    "\n",
    "for wt_ in wts:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(wt=wt_, seed=seed+i_CV)\n",
    "        \n",
    "        logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, verbose=False)\n",
    "        \n",
    "        theta0_to, wt_to, p0_to = get_params(logistic_model, verbose=False)\n",
    "        theta0_sk, wt_sk = get_params_sk(logistic_model_sk, verbose=False)\n",
    "        \n",
    "        p0s_.append(p0)\n",
    "        theta0s_.append(theta0)\n",
    "        wts_.append(wt_)\n",
    "        p0_tos.append(p0_to)\n",
    "        theta0_tos.append(theta0_to)\n",
    "        theta0_sks.append(theta0_sk)\n",
    "        wt_tos.append(wt_to)\n",
    "        wt_sks.append(wt_sk)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 8))\n",
    "\n",
    "axs[0].scatter(wts_, p0_tos, label='torch')\n",
    "axs[0].plot([min(wts_), max(wts_)], [p0, p0], '--')\n",
    "\n",
    "axs[0].set(xlabel='slope (true)', ylabel='p0 (predicted)')\n",
    "axs[0].legend(loc=\"upper left\")\n",
    "\n",
    "axs[1].scatter(wts_, theta0_tos, label='torch')\n",
    "axs[1].scatter(wts_, theta0_sks, label='sklearn')\n",
    "axs[1].plot([min(wts_), max(wts_)], [theta0, theta0], '--')\n",
    "axs[1].set(xlabel='slope (true)', ylabel='theta0 (predicted)')\n",
    "axs[1].legend(loc=\"upper left\")\n",
    "\n",
    "axs[2].scatter(wts_, wt_tos, label='torch')\n",
    "axs[2].scatter(wts_, wt_sks, label='sklearn')\n",
    "axs[2].plot([min(wts_), max(wts_)], [min(wts_), max(wts_)], '--')\n",
    "axs[2].set(xlabel='slope (true)', ylabel='slope (predicted)')\n",
    "axs[2].legend(loc=\"upper left\")\n",
    "plt.tight_layout();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "nteract": {
   "version": "0.22.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
