{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to compare the results of the fit to data with psychometric curves using logistic regression whenever one does the fit on different conditions *independently* or *jointly*. This notebook was done in collaboration with [Jenna Fradin](https://github.com/jennafradin), master student in the lab.\n",
    "\n",
    "> tl; dr = Do your fits jointly on all conditions.\n",
    "\n",
    "The rationale is that some variable which are fit are by construction similar across conditions, for instance the before-mentioned *lapse rate*, that is the frequency with which you just *miss the key*. Knowing this constraint . This is one of the aspects we will evaluate here.\n",
    "\n",
    "In this notebook, I define a fitting method using [pytorch](https://pytorch.org/) which fits in a few lines of code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "torch.set_default_tensor_type(\"torch.DoubleTensor\")\n",
    "criterion = torch.nn.BCELoss(reduction=\"sum\")\n",
    "\n",
    "N_conditions = 5\n",
    "bias = True\n",
    "logit0_init, theta0_init, log_wt_init = -2.0, 0.0, 0.1\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, logit0, theta0, log_wt, bias=True):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        if bias:\n",
    "            self.theta0 = torch.nn.Parameter(theta0)\n",
    "        else:\n",
    "            self.theta0 = theta0\n",
    "        self.logit0 = torch.nn.Parameter(logit0)\n",
    "        self.log_wt = torch.nn.Parameter(log_wt)\n",
    "        self.do_indep = self.logit0.shape.numel() > 1\n",
    "        \n",
    "    def forward(self, theta, i_condition):\n",
    "        if self.do_indep:\n",
    "            p0 = torch.sigmoid(self.logit0[i_condition.to(int)])\n",
    "        else:\n",
    "            p0 = torch.sigmoid(self.logit0)\n",
    "            \n",
    "        theta0 = self.theta0[i_condition.to(int)]\n",
    "        wt = torch.exp(self.log_wt[i_condition.to(int)])\n",
    "        out = p0 / 2 + (1 - p0) * torch.sigmoid((theta-theta0)/wt)\n",
    "        return out\n",
    "\n",
    "learning_rate = 0.005\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "betas = (beta1, beta2)\n",
    "num_epochs = 2 ** 9 + 1\n",
    "\n",
    "def fit_data(\n",
    "    theta, ind_condition, y,\n",
    "    logit0=logit0_init * torch.ones(1), theta0=theta0_init*torch.ones(N_conditions), \n",
    "    log_wt=torch.log(log_wt_init*torch.ones(N_conditions)), \n",
    "    bias=bias,\n",
    "    learning_rate=learning_rate,\n",
    "    # batch_size=batch_size,  # previous notebook showed that learning_rate had no influence on performance\n",
    "    num_epochs=num_epochs,\n",
    "    betas=betas,\n",
    "    verbose=False, **kwargs\n",
    "):\n",
    "\n",
    "    Theta, Ind_condition, labels = torch.Tensor(theta[:, None]), torch.ByteTensor(ind_condition[:, None]), torch.Tensor(y[:, None])\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    Theta, Ind_condition, labels = Theta.to(device), Ind_condition.to(device), labels.to(device)        \n",
    "    \n",
    "    logistic_model = LogisticRegressionModel(logit0, theta0, log_wt, bias=bias)\n",
    "    logistic_model = logistic_model.to(device)\n",
    "    logistic_model.train()\n",
    "    optimizer = torch.optim.Adam(logistic_model.parameters(), lr=learning_rate, betas=betas)\n",
    "    losses = []\n",
    "            \n",
    "    for epoch in range(int(num_epochs)):\n",
    "        if indep:\n",
    "            loss = 0.\n",
    "            for i_condition in range(N_conditions):\n",
    "                # if verbose: print(f'-> i_condition={i_condition}')\n",
    "                mask = Ind_condition==i_condition\n",
    "                outputs = logistic_model(Theta[mask], Ind_condition[mask])\n",
    "                loss = criterion(outputs, labels[mask])\n",
    "\n",
    "        else:\n",
    "            outputs = logistic_model(Theta, Ind_condition)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if verbose and (epoch % (num_epochs // 32) == 0):\n",
    "            print(f\"Iteration: {epoch} - Loss: {np.sum(losses)/len(theta):.5f}\")\n",
    "            losses = []\n",
    "\n",
    "    logistic_model.eval()\n",
    "    Theta, Ind_condition, labels = torch.Tensor(theta[:, None]), torch.Tensor(ind_condition[:, None]), torch.Tensor(y[:, None])\n",
    "    outputs = logistic_model(Theta, Ind_condition)\n",
    "    loss = criterion(outputs, labels).item() / len(theta)\n",
    "    return logistic_model, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and run a series of tests to compare both methods.\n",
    "\n",
    "<!-- TEASER_END -->\n",
    "\n",
    "Let's first initialize the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "# print(rcParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 20\n",
    "rcParams[\"font.size\"] = fontsize\n",
    "rcParams[\"legend.fontsize\"] = fontsize\n",
    "rcParams[\"axes.labelsize\"] = fontsize\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hyper parameters which we will tune later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256 * N_conditions\n",
    "\n",
    "N_cv = 8\n",
    "\n",
    "seed = 1973\n",
    "N_scan = 9\n",
    "N_test = N * 8 # number of points for validation\n",
    "\n",
    "bias = True\n",
    "\n",
    "p0 = 0.1\n",
    "theta0 = 0.0 * np.ones(N_conditions)\n",
    "wt = np.linspace(np.pi / 32, np.pi / 8, N_conditions)\n",
    "\n",
    "theta_std = np.pi / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## problem statement: a 2aFC task on synthetic data\n",
    "\n",
    "We will generate a typical setup where we have to guess for the otientation of a visual display compared to the vertical and ask observer to either press on the `left` or `right` arrows. The visual display will be controlled by a $theta$ parameter which we draw randomly according to a Gaussian probability density function. This may be synthesized in the following *psychometric* function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psychometric_function(theta, p0=p0, theta0=theta0, wt=wt):\n",
    "    return p0 / 2 + (1 - p0) / (1 + np.exp(-(theta - theta0) / wt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "such that we can draw the data according to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(N=N, p0=p0, theta0=theta0, wt=wt, theta_std=theta_std, N_conditions=N_conditions, seed=seed, **kwargs):\n",
    "    np.random.seed(seed)\n",
    "    theta = (2 * np.random.rand(N) - 1) * theta_std\n",
    "    ind_condition = np.random.randint(low=0, high=N_conditions, size=(N,))\n",
    "\n",
    "    p = psychometric_function(theta, p0, theta0[ind_condition], wt[ind_condition])\n",
    "\n",
    "    y = np.random.rand(N) < p  # generate data\n",
    "    \n",
    "    return theta, ind_condition, p, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, ind_condition, p, y = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127 µs ± 10.6 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "theta, ind_condition, p, y = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 - Loss: 0.39064\n",
      "Iteration: 16 - Loss: 6.17542\n",
      "Iteration: 32 - Loss: 6.09454\n",
      "Iteration: 48 - Loss: 6.03734\n",
      "Iteration: 64 - Loss: 5.99192\n",
      "Iteration: 80 - Loss: 5.95636\n",
      "Iteration: 96 - Loss: 5.92779\n",
      "Iteration: 112 - Loss: 5.90388\n",
      "Iteration: 128 - Loss: 5.88290\n",
      "Iteration: 144 - Loss: 5.86371\n",
      "Iteration: 160 - Loss: 5.84571\n",
      "Iteration: 176 - Loss: 5.82865\n",
      "Iteration: 192 - Loss: 5.81252\n",
      "Iteration: 208 - Loss: 5.79745\n",
      "Iteration: 224 - Loss: 5.78368\n",
      "Iteration: 240 - Loss: 5.77147\n",
      "Iteration: 256 - Loss: 5.76100\n",
      "Iteration: 272 - Loss: 5.75235\n",
      "Iteration: 288 - Loss: 5.74545\n",
      "Iteration: 304 - Loss: 5.74012\n",
      "Iteration: 320 - Loss: 5.73608\n",
      "Iteration: 336 - Loss: 5.73307\n",
      "Iteration: 352 - Loss: 5.73085\n",
      "Iteration: 368 - Loss: 5.72921\n",
      "Iteration: 384 - Loss: 5.72800\n",
      "Iteration: 400 - Loss: 5.72711\n",
      "Iteration: 416 - Loss: 5.72644\n",
      "Iteration: 432 - Loss: 5.72595\n",
      "Iteration: 448 - Loss: 5.72558\n",
      "Iteration: 464 - Loss: 5.72531\n",
      "Iteration: 480 - Loss: 5.72510\n",
      "Iteration: 496 - Loss: 5.72495\n",
      "Iteration: 512 - Loss: 5.72483\n",
      "Final loss = 0.3577988192419679\n"
     ]
    }
   ],
   "source": [
    "theta, ind_condition, p, y = get_data()\n",
    "logistic_model, loss = fit_data(theta, ind_condition, y, verbose=True)\n",
    "print(\"Final loss =\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That method is fairly quick, in approx 2 seconds on my MacBook Pro (Early 2015) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399 ms ± 37 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "logistic_model, loss = fit_data(theta, ind_condition, y, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare now the retrieved values. Remember the true values used to generate the data are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0 = 0.100, theta0 = 0.000, 0.000, 0.000, 0.000, 0.000, wt = 0.098, 0.172, 0.245, 0.319, 0.393, theta_std = 1.571\n"
     ]
    }
   ],
   "source": [
    "def npa2str(npa):\n",
    "    \"\"\"\n",
    "    format a numpy array into a string\n",
    "    \"\"\"\n",
    "    return ', '.join(list(map('{:.3f}'.format, npa)))\n",
    "\n",
    "print(\n",
    "    f\"p0 = {p0:.3f}, theta0 = {npa2str(theta0)}, wt = {npa2str(wt)}, theta_std = {theta_std:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we get out of our method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta0 = -0.023, -0.032, 0.059, 0.016, -0.043\n",
      "slope = 0.045, 0.166, 0.313, 0.296, 0.408\n",
      "p0 = 0.095\n"
     ]
    }
   ],
   "source": [
    "def get_params(logistic_model, verbose=False):\n",
    "    theta0_ = logistic_model.theta0.detach().numpy()\n",
    "    wt_ = torch.exp(logistic_model.log_wt).detach().numpy()\n",
    "    p0_ = torch.sigmoid(logistic_model.logit0).detach().numpy()\n",
    "    N_conditions = wt_.shape[0]\n",
    "    if verbose:\n",
    "        #for i_condition in range(N_conditions):\n",
    "        #    print(f'-> i_condition={i_condition}')\n",
    "        #    if bias:\n",
    "        #        print(f\"theta0 = {theta0_[i_condition]:.3f}\")\n",
    "        #    print(f\"slope = {wt_[i_condition]:.3f}\")\n",
    "        if bias:\n",
    "            print(f\"theta0 = {npa2str(theta0_)}\")\n",
    "        print(f\"slope = {npa2str(wt_)}\")\n",
    "        print(f\"p0 = {npa2str(p0_)}\")\n",
    "    return theta0_, wt_, p0_\n",
    "\n",
    "\n",
    "theta0_, wt_, p0_ = get_params(logistic_model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's do the same thing with on every condition *independently*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (252) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-f67b993ef7b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogistic_model_indep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind_condition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogit0_init\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_conditions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final loss =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-3c3ce7bf00c1>\u001b[0m in \u001b[0;36mfit_data\u001b[0;34m(theta, ind_condition, y, logit0, theta0, log_wt, bias, indep, learning_rate, num_epochs, betas, verbose, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0;31m# if verbose: print(f'-> i_condition={i_condition}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInd_condition\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi_condition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInd_condition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-3c3ce7bf00c1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, theta, i_condition)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtheta0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mwt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_wt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mp0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtheta0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mwt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (252) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "logistic_model_indep, loss = fit_data(theta, ind_condition, y, logit0=logit0_init * torch.ones(N_conditions), indep=True, verbose=True)\n",
    "print(\"Final loss =\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit_data_sklearn() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1f210dd15b3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogistic_model_sk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_data_sklearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: fit_data_sklearn() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "logistic_model_sk, loss = fit_data_sklearn(theta, y, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we get out of this classic method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_sk(logistic_model, verbose=False):\n",
    "\n",
    "    theta0_ = -logistic_model.intercept_[0] / logistic_model.coef_[0][0]\n",
    "    wt_ = 1 / logistic_model.coef_[0][0]\n",
    "\n",
    "    if verbose:\n",
    "        if bias:\n",
    "            print(f\"theta0 = {theta0_:.3f}\")\n",
    "        print(f\"slope = {wt_:.3f}\")\n",
    "    return theta0_, wt_\n",
    "\n",
    "theta0_, wt_ = get_params_sk(logistic_model_sk, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That method is slower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "logistic_model_sk, loss = fit_data_sklearn(theta, y, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... but what is the value of few seconds after hours of having observers sitting in front of a screen looking at (often boring) visual displays? More seriously, most important is the reliability of the values which are inferred by each respective method, such that they are correctly reflecting the information contained in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qualitative comparison of methods\n",
    "\n",
    "We can synthesize this comparison by drawing a new dataset and plotting the psychometric curves which are obtained by each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, p, y = get_data()  # nouvelles données\n",
    "logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "print(f\"Training loss = {loss:.3f}\")\n",
    "logistic_model_sk, loss_sk = fit_data_sklearn(theta, y, verbose=False)\n",
    "print(f\"Training sklearn loss = {loss_sk:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.scatter(theta, y, s=6, alpha=0.4, color=\"k\", label=\"data points\")\n",
    "# ax.scatter(theta, p, s=6, alpha=.4, color = 'b', label='hidden proba')\n",
    "x_values = np.linspace(-theta_std, theta_std, 100)[:, None]\n",
    "y_values_p = psychometric_function(x_values, p0, theta0, wt)\n",
    "ax.plot(x_values, y_values_p, alpha=0.4, color=\"b\", label=\"hidden proba\")\n",
    "y_values = logistic_model(torch.Tensor(x_values)).detach().numpy()\n",
    "ax.plot(x_values, y_values, \"g\", alpha=0.7, lw=3, label=\"torch\")\n",
    "y_values_sk = logistic_model_sk.predict_proba(x_values)[:, 1]\n",
    "ax.plot(x_values, y_values_sk, \"r\", alpha=0.7, lw=3, label=\"sklearn\")\n",
    "ax.set_xlabel(r\"orientation $\\theta$\", fontsize=20)\n",
    "ax.set_yticks([0.0, 1.0])\n",
    "ax.set_yticklabels([\"Left\", \"Right\"], fontsize=20)\n",
    "plt.legend(fontsize=20, frameon=False, scatterpoints=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear discrepency between the classical logistic function obtained by logistic regression (here, the implementation by `sklearn`) and that obtained by our more detailed model. In particular, the slope is more sharp, a feature which may be important for cognitive processes. But what is the origin of this discrepancy? Is it  the numerical optimization method? Is it the generative model?\n",
    "\n",
    "This are the question we try to resolve here. First, let's retrieve a measure for how well the extracted psychometric curve represent the data: the *losses*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, p, y = get_data(N=N_test, seed=seed + N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "print(f\"Training loss = {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model_sk, loss_sk = fit_data_sklearn(theta, y, verbose=False)\n",
    "print(f\"Training sklearn loss = {loss_sk:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The units for these losses are *bits*. Indeed, they represent some value of information about the binary data \"$d$\" and the continuous psychometric curve \"$p$\". This information is equal to $\\log_2(p)$ if $d=1$ and $\\log_2(1-p)$ if $d=0$. Putting things together, we obtain the [Binary Cross Entropy](https://pytorch.org/docs/stable/nn.html#bceloss) and if we index datapoints by $k$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_k d_k \\cdot \\log_2(p_k) + (1-d_k) \\cdot \\log_2(1-p_k)\n",
    "$$\n",
    "\n",
    "You can see that this measure is a form of [Cross_entropy](https://en.wikipedia.org/wiki/Cross_entropy), adapted to binary datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The losses which were computed above are those obtained during training. Relying on this value may be a dangerous strategy as the model may be overfitting the data. We should therefore measure how the model would generalize with novel data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it hard to do with real (experimental) data which are often scarse, here we synthesized the data and we can thus compute a testing loss by drawing again a set of new data and computing the loss on that data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_torch(logistic_model, theta, p, y):\n",
    "    labels = torch.Tensor(y[:, None])\n",
    "    Theta = torch.Tensor(theta[:, None])\n",
    "    P = torch.Tensor(p[:, None])\n",
    "\n",
    "    outputs = logistic_model(Theta)\n",
    "    return criterion(outputs, labels).item() / len(theta)\n",
    "\n",
    "\n",
    "print(f\"Testing loss = {loss_torch(logistic_model, theta, p, y):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_sklearn(logistic_model, theta, p, y):\n",
    "    outputs = logistic_model.predict_proba(theta[:, None])[:, 1]\n",
    "    outputs_, labels = torch.Tensor(outputs[:, None]), torch.Tensor(y[:, None])\n",
    "    return criterion(outputs_, labels).item() / len(theta)\n",
    "\n",
    "\n",
    "print(f\"Testing sklearn loss = {loss_sklearn(logistic_model_sk, theta, p, y):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as we synthesized the data, we can compute the loss on that data with the \"true\" psychometric curve, giving to us the baseline number one can achieve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_true(theta, p, y):\n",
    "    labels = torch.Tensor(y[:, None])\n",
    "    P = torch.Tensor(p[:, None])\n",
    "    return criterion(P, labels).item() / len(theta)\n",
    "\n",
    "\n",
    "print(f\"Testing true loss = {loss_true(theta, p, y):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We are now equipped to make quantitative comparisons. Let's first explore the parameters of the methods.\n",
    "\n",
    "## quantitative comparison of methods : varrying methods' parameters\n",
    "\n",
    "Let's study the influence of each method's meta-parameter, such as the number of iterations. As I learn to use [python decorators](https://realpython.com/primer-on-python-decorators/#a-few-real-world-examples), I will use plenty of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import time\n",
    "\n",
    "def timer(func):\n",
    "    \"\"\"Print the runtime of the decorated function\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper_timer(*args, **kwargs):\n",
    "        start_time = time.perf_counter()    # 1\n",
    "        results = func(*args, **kwargs)\n",
    "        end_time = time.perf_counter()      # 2\n",
    "        run_time = end_time - start_time    # 3\n",
    "        print(f\"Finished {func.__name__!r} in {run_time:.4f} secs\")\n",
    "        return results\n",
    "    return wrapper_timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dict = dict(learning_rate=learning_rate,\n",
    "                    num_epochs=num_epochs,\n",
    "                    betas=betas,\n",
    "                   N=N, p0=p0, theta0=theta0, wt=wt, theta_std=theta_std, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def explore_param(default_dict, variable, var_range, do_fit=True, do_SKL=True):\n",
    "    results = np.zeros((4, len(var_range), N_cv))\n",
    "    timings = []\n",
    "\n",
    "    for i_var, var_ in enumerate(var_range):\n",
    "        kwarg = default_dict.copy()\n",
    "        kwarg[variable] = var_\n",
    "        kwarg['verbose'] = False\n",
    "\n",
    "        for i_CV in range(N_cv):\n",
    "            \n",
    "            kwarg.update(seed=seed + i_CV)\n",
    "            theta, p, y = get_data(**kwarg)\n",
    "\n",
    "            tic = time.time()\n",
    "            if do_fit: logistic_model, loss = fit_data(theta, y, **kwarg)\n",
    "            if do_SKL: logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, **kwarg)\n",
    "            toc = time.time()\n",
    "\n",
    "            if N_test > 0:\n",
    "                kwarg.update(N=N_test, seed=seed + i_CV + N_test)\n",
    "                theta, p, y = get_data(**kwarg)\n",
    "                if do_fit: loss = loss_torch(logistic_model, theta, p, y)\n",
    "                if do_SKL: loss_SKL = loss_sklearn(logistic_model_sk, theta, p, y)\n",
    "\n",
    "            results[0, i_var, i_CV] = loss_true(theta, p, y)\n",
    "            if do_fit: results[1, i_var, i_CV] = loss\n",
    "            if do_SKL: results[2, i_var, i_CV] = loss_SKL\n",
    "            results[3, i_var, i_CV] = toc-tic\n",
    "            \n",
    "        print(f\"{variable}: {var_:.5f}, Loss: {np.mean(results[1, i_var, :]):.5f}, loss_P: {np.mean(results[0, i_var, :]):.5f}\")\n",
    "\n",
    "    return results\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = learning_rate * np.logspace(-2, 1, N_scan, base=10)\n",
    "results = explore_param(default_dict, variable='learning_rate', var_range=learning_rates, do_SKL=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_explore_param(variable, var_range, results):\n",
    "    opts_scat = dict(marker=\".\", lw=0, alpha=3 / N_cv, ms=20)\n",
    "    opts_line = dict(lw=1, alpha=1)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    if results[1, ...].sum()>0: ax.plot(var_range, results[1, :, :]/results[0, :, :], **opts_scat, color=\"green\")\n",
    "    ax.plot(var_range, results[0, :, :]/results[0, :, :], **opts_scat, color=\"blue\")\n",
    "    if results[2, ...].sum()>0: ax.plot(var_range, results[2, :, :]/results[0, :, :], **opts_scat, color=\"red\")\n",
    "\n",
    "    if results[1, ...].sum()>0: ax.plot(var_range, np.mean(results[1, :, :]/results[0, :, :], axis=-1), **opts_line, color=\"green\", label=\"loss\")\n",
    "    ax.plot(var_range, np.mean(results[0, :, :]/results[0, :, :], axis=-1), **opts_line, color=\"blue\", label=\"true\")\n",
    "    if results[2, ...].sum()>0: ax.plot(var_range, np.mean(results[2, :, :]/results[0, :, :], axis=-1), **opts_line, color=\"red\", label=\"loss_SKL\")\n",
    "\n",
    "    # ax.set_xlim(np.min(learning_rates_), np.max(learning_rates_))\n",
    "\n",
    "    ax.set_xlabel(variable)\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.legend(loc=\"best\")\n",
    "    return fig, ax\n",
    "\n",
    "fig, ax = plot_explore_param(variable='learning_rate', var_range=learning_rates, results=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are at a sweet spot with our learning rate, still it is valid on a wide range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence du nombre d'epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochss = num_epochs * np.logspace(-2, 1, N_scan, base=10)\n",
    "num_epochss = [int(num_epochs_) for num_epochs_ in num_epochss]\n",
    "\n",
    "results = explore_param(default_dict, variable='num_epochs', var_range=num_epochss, do_SKL=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_explore_param(variable='num_epochs', var_range=num_epochss, results=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we are at a sweet spot with our number of epochs, and this is true for both methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of `beta1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1s = 1.0 - np.logspace(-3, -2, N_scan, base=10, endpoint=True)\n",
    "results = explore_param(default_dict, variable='beta1', var_range=beta1s, do_SKL=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_explore_param(variable='beta1', var_range=beta1s, results=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The influence of this parameter is limited, such that using `Adam` is perhaps overkill. A simple `SGD` should be tested. Similarly, for `beta2`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of `beta2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta2s = 1.0 - np.logspace(-7, -2, N_scan, base=10, endpoint=True)\n",
    "results = explore_param(default_dict, variable='beta2', var_range=beta2s, do_SKL=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_explore_param(variable='beta2', var_range=beta2s, results=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of `C`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = C * np.logspace(-2, 2, N_scan, base=4)\n",
    "results = explore_param(default_dict, variable='C', var_range=Cs, do_fit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_explore_param(variable='C', var_range=Cs, results=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of `tol`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tols = tol * np.logspace(-2, 2, N_scan, base=10)\n",
    "results = explore_param(default_dict, variable='tol', var_range=tols, do_fit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_explore_param(variable='tol', var_range=tols, results=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## quantitative comparison of methods : varrying experimental parameters\n",
    "\n",
    "Now that we now more about methodological parameters, let's study more crucial parameters like that of the experiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of number of trials\n",
    "\n",
    "The number of trials is crucial as if defines the number of our datapoints, and also the length of the experiment. This is important as we want to make this number as low as possible. Indeed, observers doing the experiment, for example a master student in front of the computer screen, may experience fatigue which would prevent accurate recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = np.logspace(1.5, 3, N_scan, base=10, endpoint=True)\n",
    "Ns = [int(Ns_) for Ns_ in Ns]\n",
    "\n",
    "results = explore_param(default_dict, variable='N', var_range=Ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_explore_param(variable='N', var_range=Ns, results=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a consequence, $20$ trials is not enough and $100$ is OK. This depends on our expectations on the the retrieved data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of theta_std\n",
    "\n",
    "The convergence of the fitting procedure may also depend on the parametrers of the data which were set to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"p0 = {p0:.3f}, theta0 = {theta0:.3f}, wt = {wt:.3f}, theta_std = {theta_std:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of this is `theta_std` and is describing the \"width\" of tested orientation values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_stds = theta_std * np.logspace(-1, 1, N_scan, base=2, endpoint=True)\n",
    "\n",
    "results = explore_param(default_dict, variable='theta_std', var_range=theta_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_explore_param(variable='theta_std', var_range=theta_stds, results=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of  `theta_std` as a clear influence on the loss in particular for classical logistic regression (`sklearn`) which will have a problem with datapoints caused by the lapse rate `p0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### influence of `p0`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0s = np.logspace(-3, -0.7, N_scan, base=10, endpoint=True)\n",
    "\n",
    "results = explore_param(default_dict, variable='p0', var_range=p0s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_explore_param(variable='p0', var_range=p0s, results=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the lapse rate `p0`as an influence on the cost: when low ($p0<0.01$), cost are similar. When higher, the two methods diverge and our method is obviously better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this quantitative comparison of the methods, let's now study how the methods compare when retrieving the parameters.\n",
    "\n",
    "## comparing the predicted values\n",
    "\n",
    "In this section, we will change one parameter after antoher, while keeping the others fixed and check the retrieved value obtained by both methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"p0 = {p0:.3f}, theta0 = {theta0:.3f}, wt = {wt:.3f}, theta_std = {theta_std:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### changing `p0`\n",
    "\n",
    "Let's start by changing the lapse rate `p0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_scan = 20\n",
    "p0s = np.logspace(-3, -0.7, N_scan, base=10, endpoint=True)\n",
    "\n",
    "p0s_, wts_, theta0s_, p0_tos, theta0_tos, theta0_sks, wt_tos, wt_sks = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "for p0_ in p0s:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(p0=p0_, seed=seed + i_CV)\n",
    "\n",
    "        logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, verbose=False)\n",
    "\n",
    "        theta0_to, wt_to, p0_to = get_params(logistic_model, verbose=False)\n",
    "        theta0_sk, wt_sk = get_params_sk(logistic_model_sk, verbose=False)\n",
    "\n",
    "        p0s_.append(p0_)\n",
    "        theta0s_.append(theta0)\n",
    "        wts_.append(wt)\n",
    "        p0_tos.append(p0_to)\n",
    "        theta0_tos.append(theta0_to)\n",
    "        theta0_sks.append(theta0_sk)\n",
    "        wt_tos.append(wt_to)\n",
    "        wt_sks.append(wt_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 8))\n",
    "\n",
    "axs[0].scatter(p0s_, p0_tos, label=\"torch\")\n",
    "axs[0].plot([min(p0s_), max(p0s_)], [min(p0_tos), max(p0_tos)], \"--\")\n",
    "axs[0].set(xlabel=\"p0 (true)\", ylabel=\"p0 (predicted)\")\n",
    "axs[0].legend(loc=\"upper left\")\n",
    "\n",
    "axs[1].scatter(p0s_, theta0_tos, label=\"torch\")\n",
    "axs[1].scatter(p0s_, theta0_sks, label=\"sklearn\")\n",
    "axs[1].plot([min(p0s_), max(p0s_)], [theta0, theta0], \"--\")\n",
    "axs[1].set(xlabel=\"p0\", ylabel=\"theta0 (predicted)\")\n",
    "axs[1].legend(loc=\"upper left\")\n",
    "\n",
    "axs[2].scatter(p0s_, wt_tos, label=\"torch\")\n",
    "axs[2].scatter(p0s_, wt_sks, label=\"sklearn\")\n",
    "axs[2].plot([min(p0s_), max(p0s_)], [wt, wt], \"--\")\n",
    "axs[2].set(xlabel=\"p0\", ylabel=\"slope (predicted)\")\n",
    "axs[2].legend(loc=\"upper left\")\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our method is able to fairly accurately retrieve the value of the lapse rate. The errors obtained in the fitting of the other parameters are comparable forr `theta0`but are high on the slope. The slope is clearly overestimated depending on the lapse rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### changing `theta0`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_scan = 20\n",
    "theta0s = .61803 * theta_std * np.linspace(-1, 1, N_scan, endpoint=True)\n",
    "\n",
    "p0s_, wts_, theta0s_, p0_tos, theta0_tos, theta0_sks, wt_tos, wt_sks = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "for theta0_ in theta0s:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(theta0=theta0_, seed=seed + i_CV)\n",
    "\n",
    "        logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, verbose=False)\n",
    "\n",
    "        theta0_to, wt_to, p0_to = get_params(logistic_model, verbose=False)\n",
    "        theta0_sk, wt_sk = get_params_sk(logistic_model_sk, verbose=False)\n",
    "\n",
    "        p0s_.append(p0)\n",
    "        theta0s_.append(theta0_)\n",
    "        wts_.append(wt)\n",
    "        p0_tos.append(p0_to)\n",
    "        theta0_tos.append(theta0_to)\n",
    "        theta0_sks.append(theta0_sk)\n",
    "        wt_tos.append(wt_to)\n",
    "        wt_sks.append(wt_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 8))\n",
    "\n",
    "axs[0].scatter(theta0s_, p0_tos, label=\"torch\")\n",
    "axs[0].plot([min(theta0s_), max(theta0s_)], [p0, p0], \"--\", label=\"true\")\n",
    "axs[0].set(xlabel=\"theta0 (true)\", ylabel=\"p0 (predicted)\", ylim=(.0, .2))\n",
    "axs[0].legend(loc=\"upper left\")\n",
    "\n",
    "axs[1].scatter(theta0s_, theta0_tos, label=\"torch\")\n",
    "axs[1].scatter(theta0s_, theta0_sks, label=\"sklearn\")\n",
    "axs[1].plot([min(theta0s_), max(theta0s_)], [min(theta0s_), max(theta0s_)], \"--\", label=\"true\")\n",
    "axs[1].set(xlabel=\"theta0 (true)\", ylabel=\"theta0 (predicted)\")\n",
    "axs[1].legend(loc=\"upper left\")\n",
    "\n",
    "axs[2].scatter(theta0s_, wt_tos, label=\"torch\")\n",
    "axs[2].scatter(theta0s_, wt_sks, label=\"sklearn\")\n",
    "axs[2].plot([min(theta0s_), max(theta0s_)], [wt, wt], \"--\", label=\"true\")\n",
    "axs[2].set(xlabel=\"theta0 (true)\", ylabel=\"slope (predicted)\")\n",
    "axs[2].legend(loc=\"upper left\")\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### changing `wt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_scan = 20\n",
    "wts = wt * np.logspace(-1, .5, N_scan, base=4, endpoint=True)\n",
    "\n",
    "p0s_, wts_, theta0s_, p0_tos, theta0_tos, theta0_sks, wt_tos, wt_sks = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "for wt_ in wts:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(wt=wt_, seed=seed + i_CV)\n",
    "\n",
    "        logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, verbose=False)\n",
    "\n",
    "        theta0_to, wt_to, p0_to = get_params(logistic_model, verbose=False)\n",
    "        theta0_sk, wt_sk = get_params_sk(logistic_model_sk, verbose=False)\n",
    "\n",
    "        p0s_.append(p0)\n",
    "        theta0s_.append(theta0)\n",
    "        wts_.append(wt_)\n",
    "        p0_tos.append(p0_to)\n",
    "        theta0_tos.append(theta0_to)\n",
    "        theta0_sks.append(theta0_sk)\n",
    "        wt_tos.append(wt_to)\n",
    "        wt_sks.append(wt_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 8))\n",
    "\n",
    "axs[0].scatter(wts_, p0_tos, label=\"torch\")\n",
    "axs[0].plot([min(wts_), max(wts_)], [p0, p0], \"--\", label=\"true\")\n",
    "axs[0].set(xlabel=\"slope (true)\", ylabel=\"p0 (predicted)\", ylim=(.0, .3))\n",
    "axs[0].legend(loc=\"upper left\")\n",
    "\n",
    "axs[1].scatter(wts_, theta0_tos, label=\"torch\")\n",
    "axs[1].scatter(wts_, theta0_sks, label=\"sklearn\")\n",
    "axs[1].plot([min(wts_), max(wts_)], [theta0, theta0], \"--\", label=\"true\")\n",
    "axs[1].set(xlabel=\"slope (true)\", ylabel=\"theta0 (predicted)\", ylim=(-.2, .2))\n",
    "axs[1].legend(loc=\"upper left\")\n",
    "\n",
    "axs[2].scatter(wts_, wt_tos, label=\"torch\")\n",
    "axs[2].scatter(wts_, wt_sks, label=\"sklearn\")\n",
    "axs[2].plot([min(wts_), max(wts_)], [min(wts_), max(wts_)], \"--\", label=\"true\")\n",
    "axs[2].set(xlabel=\"slope (true)\", ylabel=\"slope (predicted)\")\n",
    "axs[2].legend(loc=\"upper left\")\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our method is able to fairly accurately retrieve the value of the lapse rate, but the precision decreases with the lapse rate. The errors obtained in the fitting of the other parameters are comparable forr `theta0` but are high on the slope. The slope is clearly overestimated in the case of the classical logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### changing `wt` with a null lapse rate (`p0=0`)\n",
    "\n",
    "Let's decompose the effect of using `torch` to using `sklearn`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_scan = 20\n",
    "wts = wt * np.logspace(-1, .5, N_scan, base=4, endpoint=True)\n",
    "\n",
    "p0s_, wts_, theta0s_, p0_tos, theta0_tos, theta0_sks, wt_tos, wt_sks = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "for wt_ in wts:\n",
    "    for i_CV in range(N_cv):\n",
    "        theta, p, y = get_data(wt=wt_, p0=0, seed=seed + i_CV)\n",
    "\n",
    "        logistic_model, loss = fit_data(theta, y, verbose=False)\n",
    "        logistic_model_sk, loss_SKL = fit_data_sklearn(theta, y, verbose=False)\n",
    "\n",
    "        theta0_to, wt_to, p0_to = get_params(logistic_model, verbose=False)\n",
    "        theta0_sk, wt_sk = get_params_sk(logistic_model_sk, verbose=False)\n",
    "\n",
    "        p0s_.append(p0)\n",
    "        theta0s_.append(theta0)\n",
    "        wts_.append(wt_)\n",
    "        p0_tos.append(p0_to)\n",
    "        theta0_tos.append(theta0_to)\n",
    "        theta0_sks.append(theta0_sk)\n",
    "        wt_tos.append(wt_to)\n",
    "        wt_sks.append(wt_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 8))\n",
    "\n",
    "axs[0].scatter(wts_, p0_tos, label=\"torch\")\n",
    "axs[0].plot([min(wts_), max(wts_)], [0, 0], \"--\", label=\"true\")\n",
    "axs[0].set(xlabel=\"slope (true)\", ylabel=\"p0 (predicted)\")\n",
    "axs[0].legend(loc=\"upper left\")\n",
    "\n",
    "axs[1].scatter(wts_, theta0_tos, label=\"torch\")\n",
    "axs[1].scatter(wts_, theta0_sks, label=\"sklearn\")\n",
    "axs[1].plot([min(wts_), max(wts_)], [theta0, theta0], \"--\", label=\"true\")\n",
    "axs[1].set(xlabel=\"slope (true)\", ylabel=\"theta0 (predicted)\", ylim=(-.2, .2))\n",
    "axs[1].legend(loc=\"upper left\")\n",
    "\n",
    "axs[2].scatter(wts_, wt_tos, label=\"torch\")\n",
    "axs[2].scatter(wts_, wt_sks, label=\"sklearn\")\n",
    "axs[2].plot([min(wts_), max(wts_)], [min(wts_), max(wts_)], \"--\", label=\"true\")\n",
    "axs[2].set(xlabel=\"slope (true)\", ylabel=\"slope (predicted)\")\n",
    "axs[2].legend(loc=\"upper left\")\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This again shows that the slope is better matched with the method presented in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -i -h -m -v -p numpy,torch,sklearn,matplotlib  -r -g -b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "nteract": {
   "version": "0.22.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
